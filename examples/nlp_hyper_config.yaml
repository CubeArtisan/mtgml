CardTextModel: !hcv
  help: The generator that replaces some tokens with likely replacements.
  value: !hc
    EncodeTokens: !hcv
      help: Encodes the sampled card tokens.
      value: !hc
        Transformer_0: !hcv
          help: The 0th transformer layer.
          value: !hc
            FinalMLP: !hcv
              help: The final transformation.
              value: !hc {}
        Transformer_1: !hcv
          help: The 1th transformer layer.
          value: !hc
            FinalMLP: !hcv
              help: The final transformation.
              value: !hc {}
        Transformer_2: !hcv
          help: The 2th transformer layer.
          value: !hc
            FinalMLP: !hcv
              help: The final transformation.
              value: !hc {}
        Transformer_3: !hcv
          help: The 3th transformer layer.
          value: !hc
            FinalMLP: !hcv
              help: The final transformation.
              value: !hc {}
        Transformer_4: !hcv
          help: The 4th transformer layer.
          value: !hc
            FinalMLP: !hcv
              help: The final transformation.
              value: !hc {}
        Transformer_5: !hcv
          help: The 5th transformer layer.
          value: !hc
            FinalMLP: !hcv
              help: The final transformation.
              value: !hc {}
        activation: !hcv
          choices: &id001 !!python/tuple
          - elu
          - selu
          - relu
          - tanh
          - sigmoid
          - linear
          - swish
          help: The activation function on the output of the layer.
          value: selu
        attention dropout_rate: !hcv
          help: The dropout rate for the attention layers of the transformer blocks.
          value: 0.1
        dense dropout_rate: !hcv
          help: The dropout rate for the dense layers of the transformer blocks.
          value: 0.2
        dims: !hcv
          help: The number of dimensions for the output.
          value: 512
        key_dims: !hcv
          help: Size of the attention head for query and key.
          value: 64
        num_heads: !hcv
          help: The number of separate heads of attention to use.
          value: 2
        num_hidden_dense: !hcv
          help: The number of hidden dense layers
          value: 1
        num_hidden_layers: !hcv
          help: Number of transformer blocks.
          value: 5
        output_dims: !hcv
          help: The number of output dimensions from this layer.
          value: 128
        use_bias: !hcv
          help: Use bias in the dense layers
          value: true
        value_dims: !hcv
          help: Size of the attention head for value.
          value: 64
    MaskTokens: !hcv
      help: The amount of tokens to replace with the MASK token.
      value: !hc
        rate: !hcv
          help: The mean proportion of tokens to replace with the MASK token.
          value: 0.65
    ReconstructTokens: !hcv
      help: Try to figure out the original identity of each token.
      value: !hc
        ToEmbeddingDims: !hcv
          help: The dense layer to get the dims the same as the embeddings
          value: !hc
            activation: !hcv
              choices: *id001
              help: The activation function on the output of the layer.
              value: linear
            use_bias: !hcv
              help: Whether to use bias on the output.
              value: true
CubeTransformFirstCard: !hcv
  help: Transform the first card in the pair.
  value: !hc
    use_bias: !hcv
      help: Whether to use bias on the output.
      value: true
CubeTransformSecondCard: !hcv
  help: Transform the first card in the pair.
  value: !hc
    use_bias: !hcv
      help: Whether to use bias on the output.
      value: true
DeckTransformFirstCard: !hcv
  help: Transform the first card in the pair.
  value: !hc
    use_bias: !hcv
      help: Whether to use bias on the output.
      value: true
DeckTransformSecondCard: !hcv
  help: Transform the first card in the pair.
  value: !hc
    use_bias: !hcv
      help: Whether to use bias on the output.
      value: true
DowncastEmbeds: !hcv
  help: Transform the second card in the pair.
  value: !hc
    dims: !hcv
      help: The number of dimensions in the output of this layer.
      value: 128
    use_bias: !hcv
      help: Whether to use bias on the output.
      value: true
ExtraLayers: !hcv
  help: The extra layers for learning card embeddings.
  value: !hc
    Transformer_0: !hcv
      help: The 0th transformer layer.
      value: !hc
        FinalMLP: !hcv
          help: The final transformation.
          value: !hc {}
    Transformer_1: !hcv
      help: The 1th transformer layer.
      value: !hc
        FinalMLP: !hcv
          help: The final transformation.
          value: !hc {}
    Transformer_2: !hcv
      help: The 2th transformer layer.
      value: !hc
        FinalMLP: !hcv
          help: The final transformation.
          value: !hc {}
    activation: !hcv
      choices: []
      help: The activation function on the output of the layer.
      value: selu
    attention dropout_rate: !hcv
      help: The dropout rate for the attention layers of the transformer blocks.
      value: 0.1
    dense dropout_rate: !hcv
      help: The dropout rate for the dense layers of the transformer blocks.
      value: 0.1
    dims: !hcv
      help: The number of dimensions for the output.
      value: 512
    key_dims: !hcv
      help: Size of the attention head for query and key.
      value: 64
    num_heads: !hcv
      help: The number of separate heads of attention to use.
      value: 2
    num_hidden_dense: !hcv
      help: The number of hidden dense layers
      value: 1
    num_hidden_layers: !hcv
      help: Number of transformer blocks.
      value: 2
    output_dims: !hcv
      help: The number of output dimensions from this layer.
      value: 128
    use_bias: !hcv
      help: Use bias in the dense layers
      value: true
    value_dims: !hcv
      help: Size of the attention head for value.
      value: 64
PositionalEmbeddings: !hcv
  help: The embeddings for the positions.
  value: !hc
    trainable: !hcv
      help: Whether the embeddings should be trained.
      value: true
TokenEmbeddings: !hcv
  help: The embeddings for the tokens.
  value: !hc
    trainable: !hcv
      help: Whether the embeddings should be trained.
      value: true
dtype: !hcv
  choices: !!python/tuple
  - 16
  - 32
  - 64
  help: The size of the floating point numbers to use for calculations in the model
  value: 32
embed_dims: !hcv
  help: The number of dimensions for the token embeddings
  value: 512
epochs_for_completion: !hcv
  help: The number of epochs it shoudl take to go through the entire dataset.
  value: 100
fine_tuning: !hcv
  help: Whether to run fine tuning on the model now.
  value: true
lamb_learning_rate: !hcv
  help: The learning rate to use for adam
  value: 0.0001
lamb_weight_decay: !hcv
  help: The weight decay for lamb optimization per batch.
  value: 1.0e-06
optimizer: !hcv
  choices: !!python/tuple
  - adam
  - adamax
  - adadelta
  - nadam
  - sgd
  - lazyadam
  - rectadam
  - novograd
  help: The optimizer type to use for optimization
  value: lamb
use_xla: !hcv
  help: Whether to use xla to speed up calculations.
  value: false
