CardTextModel: !hcv
  help: The model for learning card representations.
  value: !hc
    EncodeTokens: !hcv
      help: Process the tokens to figure out their meaning.
      value: !hc
        Transformer_0: !hcv
          help: The 0th transformer layer.
          value: !hc
            FinalMLP: !hcv
              help: The final transformation.
              value: !hc {}
        Transformer_1: !hcv
          help: The 1th transformer layer.
          value: !hc
            FinalMLP: !hcv
              help: The final transformation.
              value: !hc {}
        activation: !hcv
          choices: &id001 !!python/tuple
          - elu
          - selu
          - relu
          - tanh
          - sigmoid
          - linear
          - swish
          help: The activation function on the output of the layer.
          value: selu
        attention dropout_rate: !hcv
          help: The dropout rate for the attention layers of the transformer blocks.
          value: 0.1
        dense dropout_rate: !hcv
          help: The dropout rate for the dense layers of the transformer blocks.
          value: 0.1
        dims: !hcv
          help: The number of dimensions for the output.
          value: 1024
        key_dims: !hcv
          help: Size of the attention head for query and key.
          value: 64
        num_heads: !hcv
          help: The number of separate heads of attention to use.
          value: 8
        num_hidden_dense: !hcv
          help: The number of hidden dense layers
          value: 0
        num_hidden_layers: !hcv
          help: Number of transformer blocks.
          value: 1
        output_dims: !hcv
          help: The number of output dimensions from this layer.
          value: 512
        use_bias: !hcv
          help: Use bias in the dense layers
          value: true
        value_dims: !hcv
          help: Size of the attention head for value.
          value: 64
    MaskTokens: !hcv
      help: The amount of tokens to replace with the MASK token.
      value: !hc
        rate: !hcv
          help: The mean proportion of tokens to replace with the MASK token.
          value: 0.15
    PositionEmbeddingTransform: !hcv
      help: Project the token embeddings down to embed_dims.
      value: !hc
        activation: !hcv
          choices: *id001
          help: The activation function on the output of the layer.
          value: selu
        use_bias: !hcv
          help: Whether to use bias on the output.
          value: true
    ReconstructTokens: !hcv
      help: Try to figure out the original identity of each token.
      value: !hc
        ToEmbeddingDims: !hcv
          help: The dense layer to get the dims the same as the embeddings
          value: !hc
            activation: !hcv
              choices: *id001
              help: The activation function on the output of the layer.
              value: selu
            use_bias: !hcv
              help: Whether to use bias on the output.
              value: true
    TokenEmbeddingTransform: !hcv
      help: Project the token embeddings down to embed_dims.
      value: !hc
        activation: !hcv
          choices: *id001
          help: The activation function on the output of the layer.
          value: selu
        use_bias: !hcv
          help: Whether to use bias on the output.
          value: true
    embed_dims: !hcv
      help: The number of dimensions to project the token embeddings down to
      value: 512
CubeAdjMtx: !hcv
  help: The model to reconstruct the cube adjacency matrix
  value: !hc
    FinalRecoverAdjMtx: !hcv
      help: The last layer for reconstructing the adj_mtx.
      value: !hc
        use_bias: !hcv
          help: Whether to use bias on the output.
          value: true
    RecoverAdjMtx: !hcv
      help: The MLP layer that tries to reconstruct the adjacency matrix row for the
        single card cube
      value: !hc
        Dropout: !hcv
          help: The dropout applied after each hidden layer.
          value: !hc
            rate: !hcv
              help: The percent of values that get replaced with zero.
              value: 0
        activation: !hcv
          choices: *id001
          help: The activation function on the output of the layer.
          value: selu
        dims: !hcv
          help: The number of dimensions for the output.
          value: 2048
        num_hidden: !hcv
          help: The number of hidden layers in the MLP.
          value: 1
        use_batch_norm: !hcv
          help: Use batch normalization between layers
          value: false
        use_bias: !hcv
          help: Whether to add on a bias at each layer.
          value: true
        use_layer_norm: !hcv
          help: Use layer normalization between layers
          value: true
DeckAdjMtx: !hcv
  help: The model to reconstruct the deck adjacency matrix
  value: !hc
    FinalRecoverAdjMtx: !hcv
      help: The last layer for reconstructing the adj_mtx.
      value: !hc
        use_bias: !hcv
          help: Whether to use bias on the output.
          value: true
    RecoverAdjMtx: !hcv
      help: The MLP layer that tries to reconstruct the adjacency matrix row for the
        single card cube
      value: !hc
        Dropout: !hcv
          help: The dropout applied after each hidden layer.
          value: !hc
            rate: !hcv
              help: The percent of values that get replaced with zero.
              value: 0
        activation: !hcv
          choices: *id001
          help: The activation function on the output of the layer.
          value: selu
        dims: !hcv
          help: The number of dimensions for the output.
          value: 2048
        num_hidden: !hcv
          help: The number of hidden layers in the MLP.
          value: 1
        use_batch_norm: !hcv
          help: Use batch normalization between layers
          value: false
        use_bias: !hcv
          help: Whether to add on a bias at each layer.
          value: true
        use_layer_norm: !hcv
          help: Use layer normalization between layers
          value: true
adam_learning_rate: !hcv
  help: The learning rate to use for adam
  value: 0.001
adj_mtx_batch_size: !hcv
  help: The number of rows of the adjacency matrices to evaluate at a time.
  value: 32
dtype: !hcv
  choices: !!python/tuple
  - 16
  - 32
  - 64
  help: The size of the floating point numbers to use for calculations in the model
  value: 32
embed_dims: !hcv
  help: The number of dimensions for the token embeddings
  value: 512
epochs_for_completion: !hcv
  help: The number of epochs it should take to go through the entire dataset.
  value: 1
optimizer: !hcv
  choices: !!python/tuple
  - adam
  - adamax
  - adadelta
  - nadam
  - sgd
  - lazyadam
  - rectadam
  - novograd
  help: The optimizer type to use for optimization
  value: adam
use_xla: !hcv
  help: Whether to use xla to speed up calculations.
  value: true
